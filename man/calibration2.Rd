% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/calibration2.R
\name{calibration2}
\alias{calibration2}
\title{Calibration, evaluation, and selection of candidate models}
\usage{
calibration2(data, test_concave = TRUE, extrapolation_factor = 0.1,
           var_limits = NULL, averages_from = "pr" ,
           addsamplestobackground = TRUE,
           use_weights = FALSE, parallel = TRUE, ncores = 4,
           parallel_type = "doSNOW", progress_bar = TRUE, write_summary = FALSE,
           out_dir = NULL, skip_existing_models = FALSE,
           return_replicate = TRUE, omission_rate= 10, omrat_threshold = 10,
           AIC = "ws", delta_aic = 2, allow_tolerance = TRUE,
           tolerance = 0.01, verbose = TRUE)
}
\arguments{
\item{data}{an object of class \code{prepare_data} returned by the
\code{\link{prepare_data()}} function. It contains the calibration data,
formulas grid, kfolds, and model type.}

\item{test_concave}{(logical) whether to test for and remove candidate models
presenting concave curves. Default is TRUE.}

\item{extrapolation_factor}{(numeric) a multiplier used to calculate the
extrapolation range for each variable when detecting concave curves. Larger
values allow broader extrapolation beyond the observed data range, while
smaller values restrict the range. Default is 0.1. See details.}

\item{var_limits}{(list) A named list specifying the lower and/or upper limits
for some variables. The first value represents the lower limit, and the
second value represents the upper limit. Default is \code{NULL}, meaning no
specific limits are applied, and the range will be calculated using the
\code{extrapolation_factor}. See details.}

\item{averages_from}{(character) specifies how the averages or modes of the
variables are calculated. Available options are "pr" (to calculate averages
from the presence localities) or "pr_bg" (to use the combined set of presence
and background localities). Default is "pr". See details.}

\item{addsamplestobackground}{(logical) whether to add to the background any
presence sample that is not already there. Default is TRUE.}

\item{use_weights}{(logical) whether to apply the weights present in the
data. Default is FALSE.}

\item{parallel}{(logical) whether to fit the candidate models in parallel.
Default is FALSE.}

\item{ncores}{(numeric) number of cores to use for parallel processing.
Default is 1. This is only applicable if \code{parallel = TRUE}.}

\item{parallel_type}{(character) the package to use for parallel processing:
"doParallel" or "doSNOW". Default is "doSNOW". This is only applicable if
\code{parallel = TRUE}.}

\item{progress_bar}{(logical) whether to display a progress bar during
processing. Default is TRUE.}

\item{write_summary}{(logical) whether to save the evaluation results for
each candidate model to disk. Default is FALSE.}

\item{out_dir}{(character) the file name, with or without a path, for saving
the evaluation results for each candidate model. This is only applicable if
\code{write_summary = TRUE}.}

\item{skip_existing_models}{(logical) whether to check for and skip candidate
models that have already been fitted and saved in \code{out_dir}. This is only
applicable if \code{write_summary = TRUE}. Default is FALSE.}

\item{return_replicate}{(logical) whether to return the evaluation results
for each replicate. Default is TRUE, meaning evaluation results for each
replicate will be returned.}

\item{omission_rate}{(numeric) values from 0 to 100 representing the
percentage of potential error due to any source of uncertainty. This value is
used to calculate the omission rate. Default is 10. See details.}

\item{omrat_threshold}{(numeric) the maximum omission rate a candidate model
can have to be considered a best model. This value must match one of the
values specified in omrat. Defaut is 10.}

\item{AIC}{(character) the type of AIC to be calculated: "ws" for AIC
proposed by Warren and Seifert (2011), or "nk" for AIC proposed by Ninomiya
and Kawano (2016). Default is "ws". See References for details.}

\item{delta_aic}{(numeric) the value of delta AIC used as a threshold to
select models. Default is 2.}

\item{allow_tolerance}{(logical) whether to allow selection of models with
minimum values of omission rates even if their omission rate surpasses the
\code{omrat_threshold}. This is only applicable if all candidate models have
omission rates higher than the \code{omrat_threshold}. Default is TRUE.}

\item{tolerance}{(numeric) The value added to the minimum omission rate if it
exceeds the \code{omrat_threshold}. If \code{allow_tolerance = TRUE}, selected models
will have an omission rate equal to or less than the minimum rate plus this
tolerance. Default is 0.01.}

\item{verbose}{(logical) whether to display messages during processing.
Default is TRUE.}

\item{...}{Additional arguments passed to \code{\link[enmpa]{proc_enm}} for
calculating partial ROC. See ?enmpa::proc_enm}
}
\value{
An object of class 'calibration_results' containing the following elements:
\itemize{
\item species: a character string with the name of the species.
\item calibration data: a data.frame containing a column (\code{pr_bg}) that
identifies occurrence points (1) and background points (0), along with the
corresponding values of predictor variables for each point.
\item formula_grid: data frame containing the calibration grid with possible
formulas and parameters.
\item kfolds: a list of vectors with row indices corresponding to each fold.
\item data_xy: a data.frame with occurrence and background coordinates.
\item continuous_variables: a character indicating the continuous variables.
\item categorical_variables: a character, categorical variable names (if used).
\item weights: a numeric vector specifying weights for data_xy (if used).
\item pca: if a principal component analysis was performed with variables, a list
of class "prcomp". See ?stats::prcomp() for details.
\item model_type: the model type (glm or glmnet)
\item calibration_results: a list containing a data frame with all evaluation
metrics for all replicates (if \code{return_replicate = TRUE}) and a summary of
the evaluation metrics for each candidate model.
\item omission_rate: The omission rate determined by \code{omrat_threshold} for
selecting best models.
\item addsamplestobackground: a logical value indicating whether any presence
sample not already in the background was added.
\item selected_models: data frame with the ID and the summary of evaluation
metrics for the selected models.
\item summary: A list containing the delta AIC values for model selection, and
the ID values of models that failed to fit, had concave curves,
non-significant pROC values, omission rates above the threshold, delta AIC
values above the threshold, and the selected models.
}
}
\description{
This function fits and validates candidate models using the data and grid of
formulas prepared with \code{\link{prepare_data}}(). It supports both \code{glm} and \code{glmnet}
model types. The function then selects the best models based on concave
curves (optional), partial ROC, omission rate, and AIC values.
}
\details{
Partial ROC is calculated following Peterson et al.
(2008; http://dx.doi.org/10.1016/j.ecolmodel.2007.11.008).

Omission rates are calculated using models trained with separate testing data
subsets. Users can specify multiple omission rates to be calculated
(e.g., c(5, 10)), though only one can be used as the threshold for selecting
the best models.

Model complexity (AIC) is assessed using models generated with the complete
set of occurrences.

Concave curves are identified by analyzing the beta coefficients of quadratic
terms within the variable's range. The range for extrapolation is calculated
as the difference between the variable's maximum and minimum values in the
model, multiplied by the extrapolation factor. A concave curve is detected
when the beta coefficient is positive, and the vertex — where the curve
changes direction — lies between the lower and upper limits of the variable.

Users can specify the lower and upper limits for certain variables using
\code{var_limits}. For example, if \code{var_limits = list("bio12" = c(0, NA),
"bio15" = c(0, 100))}, the lower limit for \code{bio12} will be 0, and the
upper limit will be calculated using the extrapolation factor. Similarly,
the lower and upper limits for \code{bio15} will be 0 and 100, respectively.

For calculating the vertex position, a response curve for a given variable is
generated with all other variables set to their mean values (or mode for
categorical variables). These values are calculated either from the presence
localities (if \code{averages_from = "pr"}) or from the combined set of
presence and background localities (if \code{averages_from = "pr_bg"}).
}
\examples{
# Import occurrences
data(occ_data, package = "kuenm2")

# Import variables
var <- terra::rast(system.file("extdata", "Current_variables.tif",
                               package = "kuenm2"))
#Remove categorical variable
var <- var[[1:4]]

#### GLMNET ####
# Prepare data for glmnet model
sp_swd <- prepare_data(model_type = "glmnet", occ = occ_data,
                       species = occ_data[1, 1], x = "x", y = "y",
                       spat_variables = var, mask = NULL,
                       categorical_variables = NULL,
                       do_pca = FALSE, deviance_explained = 95,
                       min_explained = 5, center = TRUE, scale = TRUE,
                       write_pca = FALSE, output_pca = NULL, nbg = 100,
                       kfolds = 4, weights = NULL, min_number = 3,
                       min_continuous = NULL,
                       features = c("l", "lq"),
                       regm = 1,
                       include_xy = TRUE,
                       write_file = FALSE, file_name = NULL,
                       seed = 1)

# Calibrate glmnet models
m <- calibration2(data = sp_swd,
                  test_concave = TRUE,
                  extrapolation_factor = 0.1,
                  var_limits = list("bio_7" = c(0, NA),
                                    "bio_12" = c(0, NA),
                                    "bio_15" = c(0, 100)),
                  averages_from = "pr",
                  parallel = FALSE,
                  ncores = 1,
                  progress_bar = TRUE,
                  write_summary = FALSE,
                  out_dir = NULL,
                  parallel_type = "doSNOW",
                  return_replicate = TRUE,
                  omission_rate = c(5, 10),
                  omrat_threshold = 10,
                  allow_tolerance = TRUE,
                  tolerance = 0.01,
                  AIC = "ws",
                  delta_aic = 2,
                  skip_existing_models = FALSE,
                  verbose = TRUE)
m

#### GLM ####
# Prepare data for glm model
sp_swd_glm <- prepare_data(model_type = "glm", occ = occ_data,
                           species = occ_data[1, 1], x = "x", y = "y",
                           spat_variables = var, mask = NULL,
                           categorical_variables = NULL,
                           do_pca = FALSE, deviance_explained = 95,
                           min_explained = 5, center = TRUE, scale = TRUE,
                           write_pca = FALSE, output_pca = NULL, nbg = 100,
                           kfolds = 4, weights = NULL, min_number = 3,
                           min_continuous = NULL,
                           features = c("l", "lq"),
                           regm = 1,
                           include_xy = TRUE,
                           write_file = FALSE, file_name = NULL,
                           seed = 1)

# Calibrate glm models
m_glm <- calibration2(data = sp_swd_glm,
                      test_concave = TRUE,
                      extrapolation_factor = 0.1,
                      var_limits = list("bio_7" = c(0, NA),
                                        "bio_12" = c(0, NA),
                                        "bio_15" = c(0, 100)),
                      averages_from = "pr",
                      parallel = FALSE,
                      ncores = 1,
                      progress_bar = TRUE,
                      write_summary = FALSE,
                      out_dir = NULL,
                      parallel_type = "doSNOW",
                      return_replicate = TRUE,
                      omission_rate = c(5, 10),
                      omrat_threshold = 10,
                      allow_tolerance = TRUE,
                      tolerance = 0.01,
                      AIC = "ws",
                      delta_aic = 2,
                      skip_existing_models = FALSE,
                      verbose = TRUE)
m_glm

}
\references{
Ninomiya, Yoshiyuki, and Shuichi Kawano. "AIC for the Lasso in generalized
linear models." (2016): 2537-2560.

Warren, D. L., & Seifert, S. N. (2011). Ecological niche modeling in Maxent:
the importance of model complexity and the performance of model selection
criteria. Ecological applications, 21(2), 335-342.
}
